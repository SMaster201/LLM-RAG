"""
Qwen RAG å®Œæ•´æ•ˆèƒ½æ¸¬è©¦
æ¸¬è©¦æ¨¡å‹è®€å– PDFã€å›ç­”å•é¡Œï¼Œä¸¦è¨˜éŒ„æ‰€æœ‰ VRAM å’Œæ•ˆèƒ½æŒ‡æ¨™
"""

import os
import sys
import time
import json
import torch
import gc
import psutil
from datetime import datetime
from typing import Dict, List, Any
import pandas as pd
from pathlib import Path

# å˜—è©¦å°å…¥ VRAM ç›£æ§å·¥å…·
try:
    import pynvml
    NVML_AVAILABLE = True
    pynvml.nvmlInit()
except:
    NVML_AVAILABLE = False
    print("âš ï¸ NVIDIA GPU ä¸å¯ç”¨æˆ– pynvml æœªå®‰è£ï¼Œå°‡åªè¨˜éŒ„ CPU è¨˜æ†¶é«”")

from qwen_rag_system import QwenRAGSystem


class PerformanceMonitor:
    """æ•ˆèƒ½ç›£æ§å™¨"""
    
    def __init__(self):
        self.has_gpu = NVML_AVAILABLE and torch.cuda.is_available()
        if self.has_gpu:
            self.gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)
        self.baseline_vram = 0
        self.peak_vram = 0
        self.process = psutil.Process()
        
    def get_vram_mb(self) -> float:
        """ç²å–ç•¶å‰ VRAM ä½¿ç”¨é‡ (MB)"""
        if self.has_gpu:
            try:
                info = pynvml.nvmlDeviceGetMemoryInfo(self.gpu_handle)
                return info.used / 1024 / 1024
            except:
                return 0
        else:
            # è¿”å› CPU RAM ä½¿ç”¨é‡
            return self.process.memory_info().rss / 1024 / 1024
    
    def get_cpu_memory_mb(self) -> float:
        """ç²å– CPU è¨˜æ†¶é«”ä½¿ç”¨é‡ (MB)"""
        return self.process.memory_info().rss / 1024 / 1024
    
    def set_baseline(self):
        """è¨­å®šåŸºç·š VRAM"""
        self.baseline_vram = self.get_vram_mb()
        self.peak_vram = self.baseline_vram
        
    def update_peak(self):
        """æ›´æ–°å³°å€¼ VRAM"""
        current = self.get_vram_mb()
        if current > self.peak_vram:
            self.peak_vram = current
            
    def get_stats(self) -> Dict[str, float]:
        """ç²å–çµ±è¨ˆæ•¸æ“š"""
        current = self.get_vram_mb()
        return {
            'baseline_vram_mb': round(self.baseline_vram, 2),
            'current_vram_mb': round(current, 2),
            'peak_vram_mb': round(self.peak_vram, 2),
            'vram_growth_mb': round(self.peak_vram - self.baseline_vram, 2),
            'cpu_memory_mb': round(self.get_cpu_memory_mb(), 2)
        }


def clear_memory():
    """æ¸…ç†è¨˜æ†¶é«”"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    time.sleep(2)


def test_rag_with_pdf(model_name: str, pdf_folder: str, questions: List[str]) -> Dict[str, Any]:
    """
    æ¸¬è©¦ RAG ç³»çµ±ï¼šè¼‰å…¥ PDFã€å›ç­”å•é¡Œ
    
    Args:
        model_name: æ¨¡å‹åç¨±
        pdf_folder: PDF è³‡æ–™å¤¾è·¯å¾‘
        questions: è¦å•çš„å•é¡Œåˆ—è¡¨
        
    Returns:
        åŒ…å«æ‰€æœ‰æ¸¬è©¦çµæœçš„å­—å…¸
    """
    print("=" * 80)
    print(f"æ¸¬è©¦æ¨¡å‹: {model_name}")
    print(f"PDF è³‡æ–™å¤¾: {pdf_folder}")
    print(f"å•é¡Œæ•¸é‡: {len(questions)}")
    print("=" * 80 + "\n")
    
    monitor = PerformanceMonitor()
    results = {
        'model_name': model_name,
        'pdf_folder': pdf_folder,
        'questions': questions,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        'timestamp': datetime.now().isoformat()
    }
    
    # æ¸…ç†è¨˜æ†¶é«”
    clear_memory()
    monitor.set_baseline()
    
    # 1. è¼‰å…¥æ¨¡å‹
    print("ğŸ“¥ éšæ®µ 1: è¼‰å…¥æ¨¡å‹")
    model_load_start = time.time()
    
    try:
        rag_system = QwenRAGSystem(model_name=model_name, device="auto")
        model_load_time = time.time() - model_load_start
        monitor.update_peak()
        
        results['model_load_time_sec'] = round(model_load_time, 2)
        results['model_load_success'] = True
        print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œè€—æ™‚: {model_load_time:.2f} ç§’")
        
        # è¨˜éŒ„æ¨¡å‹è¼‰å…¥å¾Œçš„ VRAM
        model_loaded_stats = monitor.get_stats()
        results['after_model_load'] = model_loaded_stats
        print(f"   VRAM: {model_loaded_stats['current_vram_mb']} MB")
        print()
        
    except Exception as e:
        results['model_load_success'] = False
        results['model_load_error'] = str(e)
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return results
    
    # 2. è¼‰å…¥ PDF æ–‡ä»¶
    print("ğŸ“š éšæ®µ 2: è¼‰å…¥ PDF æ–‡ä»¶")
    pdf_load_start = time.time()
    
    try:
        # æƒæ PDF è³‡æ–™å¤¾
        pdf_files = list(Path(pdf_folder).glob("*.pdf"))
        if not pdf_files:
            raise FileNotFoundError(f"åœ¨ {pdf_folder} ä¸­æ‰¾ä¸åˆ° PDF æ–‡ä»¶")
        
        print(f"   æ‰¾åˆ° {len(pdf_files)} å€‹ PDF æ–‡ä»¶")
        for pdf in pdf_files:
            print(f"   - {pdf.name}")
        
        # è¼‰å…¥æ–‡ä»¶åˆ° RAG ç³»çµ±
        rag_system.load_documents([str(f) for f in pdf_files])
        pdf_load_time = time.time() - pdf_load_start
        monitor.update_peak()
        
        results['pdf_count'] = len(pdf_files)
        results['pdf_files'] = [f.name for f in pdf_files]
        results['pdf_load_time_sec'] = round(pdf_load_time, 2)
        results['pdf_load_success'] = True
        print(f"âœ… PDF è¼‰å…¥å®Œæˆï¼Œè€—æ™‚: {pdf_load_time:.2f} ç§’")
        
        # è¨˜éŒ„ PDF è¼‰å…¥å¾Œçš„ VRAM
        pdf_loaded_stats = monitor.get_stats()
        results['after_pdf_load'] = pdf_loaded_stats
        print(f"   VRAM: {pdf_loaded_stats['current_vram_mb']} MB")
        print(f"   RAG é¡å¤– VRAM: {pdf_loaded_stats['current_vram_mb'] - model_loaded_stats['current_vram_mb']:.2f} MB")
        print()
        
    except Exception as e:
        results['pdf_load_success'] = False
        results['pdf_load_error'] = str(e)
        print(f"âŒ PDF è¼‰å…¥å¤±æ•—: {e}")
        return results
    
    # 3. å•ç­”æ¸¬è©¦
    print("â“ éšæ®µ 3: å•ç­”æ¸¬è©¦")
    qa_start = time.time()
    
    qa_results = []
    total_retrieval_time = 0
    total_generation_time = 0
    total_tokens = 0
    
    try:
        for i, question in enumerate(questions, 1):
            print(f"\n   å•é¡Œ {i}/{len(questions)}: {question}")
            
            # å…ˆæª¢ç´¢ç›¸é—œå…§å®¹
            retrieval_start = time.time()
            context = rag_system.retrieve_context(question, k=3)
            retrieval_time = time.time() - retrieval_start
            monitor.update_peak()
            
            # ç”Ÿæˆç­”æ¡ˆ
            generation_start = time.time()
            answer = rag_system.generate_answer(question, context)
            generation_time = time.time() - generation_start
            monitor.update_peak()
            
            # è¨ˆç®— tokensï¼ˆç²—ç•¥ä¼°è¨ˆï¼‰
            answer_tokens = len(answer.split())
            tokens_per_sec = answer_tokens / generation_time if generation_time > 0 else 0
            
            total_retrieval_time += retrieval_time
            total_generation_time += generation_time
            total_tokens += answer_tokens
            
            qa_result = {
                'question': question,
                'answer': answer,
                'retrieval_time_sec': round(retrieval_time, 2),
                'generation_time_sec': round(generation_time, 2),
                'answer_length': len(answer),
                'answer_tokens_estimate': answer_tokens,
                'tokens_per_sec': round(tokens_per_sec, 2)
            }
            qa_results.append(qa_result)
            
            print(f"   âœ“ æª¢ç´¢: {retrieval_time:.2f}ç§’ | ç”Ÿæˆ: {generation_time:.2f}ç§’ | é€Ÿåº¦: {tokens_per_sec:.2f} t/s")
            print(f"   ğŸ“ ç­”æ¡ˆ: {answer}")
        
        qa_total_time = time.time() - qa_start
        avg_tokens_per_sec = total_tokens / total_generation_time if total_generation_time > 0 else 0
        
        results['qa_results'] = qa_results
        results['total_retrieval_time_sec'] = round(total_retrieval_time, 2)
        results['total_generation_time_sec'] = round(total_generation_time, 2)
        results['qa_total_time_sec'] = round(qa_total_time, 2)
        results['avg_tokens_per_sec'] = round(avg_tokens_per_sec, 2)
        results['total_questions'] = len(questions)
        results['qa_success'] = True
        
        print(f"\nâœ… æ‰€æœ‰å•ç­”å®Œæˆï¼Œç¸½è€—æ™‚: {qa_total_time:.2f} ç§’")
        print(f"   å¹³å‡ç”Ÿæˆé€Ÿåº¦: {avg_tokens_per_sec:.2f} tokens/ç§’")
        
        # è¨˜éŒ„å•ç­”å¾Œçš„ VRAM
        qa_stats = monitor.get_stats()
        results['after_qa'] = qa_stats
        print(f"   å³°å€¼ VRAM: {qa_stats['peak_vram_mb']} MB")
        print()
        
    except Exception as e:
        results['qa_success'] = False
        results['qa_error'] = str(e)
        print(f"âŒ å•ç­”å¤±æ•—: {e}")
        return results
    
    # 4. æœ€çµ‚çµ±è¨ˆ
    final_stats = monitor.get_stats()
    results['final_stats'] = final_stats
    results['total_vram_growth_mb'] = final_stats['vram_growth_mb']
    
    # RAG æˆæœ¬è©•ä¼°
    if 'after_model_load' in results and 'after_pdf_load' in results:
        rag_overhead = results['after_pdf_load']['current_vram_mb'] - results['after_model_load']['current_vram_mb']
        results['rag_overhead_mb'] = round(rag_overhead, 2)
    
    print("=" * 80)
    print("ğŸ“Š æ¸¬è©¦å®Œæˆ")
    print("=" * 80)
    
    return results


def generate_excel_report(results_list: List[Dict[str, Any]], output_file: str):
    """
    ç”Ÿæˆ Excel å ±å‘Š
    
    Args:
        results_list: æ¸¬è©¦çµæœåˆ—è¡¨
        output_file: è¼¸å‡º Excel æª”æ¡ˆè·¯å¾‘
    """
    print(f"\nğŸ“Š ç”Ÿæˆ Excel å ±å‘Š: {output_file}")
    
    # æº–å‚™ä¸»è¦æ•¸æ“šè¡¨
    main_data = []
    for r in results_list:
        if not r.get('qa_success'):
            continue
            
        row = {
            'æ¨¡å‹ç‰ˆæœ¬': r['model_name'],
            'åŸ·è¡Œè¨­å‚™': r['device'],
            'åŸ·è¡Œä»»å‹™é¡å‹': 'RAG å•ç­”',
            'PDF æ•¸é‡': r.get('pdf_count', 0),
            'å•é¡Œæ•¸é‡': r.get('total_questions', 0),
            'æ¨¡å‹è¼‰å…¥æ™‚é–“ (ç§’)': r.get('model_load_time_sec', 0),
            'PDF è¼‰å…¥æ™‚é–“ (ç§’)': r.get('pdf_load_time_sec', 0),
            'ç¸½æª¢ç´¢æ™‚é–“ (ç§’)': r.get('total_retrieval_time_sec', 0),
            'ç¸½ç”Ÿæˆæ™‚é–“ (ç§’)': r.get('total_generation_time_sec', 0),
            'ç¸½è€—æ™‚ (ç§’)': r.get('qa_total_time_sec', 0),
            'éœç½® VRAM (MB)': r['final_stats']['baseline_vram_mb'],
            'å³°å€¼ VRAM (MB)': r['final_stats']['peak_vram_mb'],
            'VRAM å¢é•·é‡ (MB)': r['final_stats']['vram_growth_mb'],
            'RAG é¡å¤– VRAM (MB)': r.get('rag_overhead_mb', 0),
            'CPU è¨˜æ†¶é«” (MB)': r['final_stats']['cpu_memory_mb'],
            'å¹³å‡ç”Ÿæˆé€Ÿåº¦ (tokens/ç§’)': r.get('avg_tokens_per_sec', 0),
            'æ¸¬è©¦æ™‚é–“': r['timestamp']
        }
        main_data.append(row)
    
    # å‰µå»º Excel writer
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        # ä¸»è¦æ•¸æ“šè¡¨
        if main_data:
            df_main = pd.DataFrame(main_data)
            df_main.to_excel(writer, sheet_name='æ•ˆèƒ½æ¸¬è©¦çµæœ', index=False)
        
        # VRAM éšæ®µåˆ†æ
        vram_data = []
        for r in results_list:
            if not r.get('qa_success'):
                continue
            
            stages = [
                ('åŸºç·š', r['final_stats']['baseline_vram_mb']),
                ('æ¨¡å‹è¼‰å…¥å¾Œ', r.get('after_model_load', {}).get('current_vram_mb', 0)),
                ('PDF è¼‰å…¥å¾Œ', r.get('after_pdf_load', {}).get('current_vram_mb', 0)),
                ('å•ç­”å¾Œå³°å€¼', r['final_stats']['peak_vram_mb'])
            ]
            
            for stage_name, vram in stages:
                vram_data.append({
                    'æ¨¡å‹': r['model_name'],
                    'éšæ®µ': stage_name,
                    'VRAM (MB)': vram
                })
        
        if vram_data:
            df_vram = pd.DataFrame(vram_data)
            df_vram.to_excel(writer, sheet_name='VRAM éšæ®µåˆ†æ', index=False)
        
        # RAG æˆæœ¬è©•ä¼°
        rag_cost_data = []
        for r in results_list:
            if not r.get('qa_success'):
                continue
            
            rag_cost_data.append({
                'æ¨¡å‹': r['model_name'],
                'åŸºç¤æ¨¡å‹ VRAM (MB)': r.get('after_model_load', {}).get('current_vram_mb', 0),
                'åŠ å…¥ RAG å¾Œ VRAM (MB)': r.get('after_pdf_load', {}).get('current_vram_mb', 0),
                'RAG é¡å¤–æˆæœ¬ (MB)': r.get('rag_overhead_mb', 0),
                'RAG æˆæœ¬æ¯”ä¾‹ (%)': round(r.get('rag_overhead_mb', 0) / r.get('after_model_load', {}).get('current_vram_mb', 1) * 100, 2) if r.get('after_model_load', {}).get('current_vram_mb', 0) > 0 else 0,
                'PDF æ•¸é‡': r.get('pdf_count', 0),
                'PDF è¼‰å…¥æ™‚é–“ (ç§’)': r.get('pdf_load_time_sec', 0)
            })
        
        if rag_cost_data:
            df_rag = pd.DataFrame(rag_cost_data)
            df_rag.to_excel(writer, sheet_name='RAG æˆæœ¬è©•ä¼°', index=False)
        
        # é€Ÿåº¦åˆ†æ
        speed_data = []
        for r in results_list:
            if not r.get('qa_success'):
                continue
            
            speed_data.append({
                'æ¨¡å‹': r['model_name'],
                'åŸ·è¡Œè¨­å‚™': r['device'],
                'å•é¡Œæ•¸é‡': r.get('total_questions', 0),
                'ç¸½æª¢ç´¢æ™‚é–“ (ç§’)': r.get('total_retrieval_time_sec', 0),
                'ç¸½ç”Ÿæˆæ™‚é–“ (ç§’)': r.get('total_generation_time_sec', 0),
                'ç¸½éŸ¿æ‡‰æ™‚é–“ (ç§’)': r.get('qa_total_time_sec', 0),
                'å¹³å‡ç”Ÿæˆé€Ÿåº¦ (tokens/ç§’)': r.get('avg_tokens_per_sec', 0),
                'æ˜¯å¦æ»¿è¶³å³æ™‚éœ€æ±‚': 'æ˜¯' if r.get('qa_total_time_sec', 999) < 10 else 'å¦',
                'é€Ÿåº¦è©•ç´š': 'å¿«' if r.get('avg_tokens_per_sec', 0) > 20 else 'ä¸­' if r.get('avg_tokens_per_sec', 0) > 10 else 'æ…¢'
            })
        
        if speed_data:
            df_speed = pd.DataFrame(speed_data)
            df_speed.to_excel(writer, sheet_name='é€Ÿåº¦åˆ†æ', index=False)
        
        # å•é¡Œèˆ‡ç­”æ¡ˆ
        qa_data = []
        for r in results_list:
            if not r.get('qa_success'):
                continue
            
            # ç‚ºæ¯å€‹å•é¡Œå‰µå»ºä¸€è¡Œ
            for qa in r.get('qa_results', []):
                qa_data.append({
                    'æ¨¡å‹': r['model_name'],
                    'å•é¡Œ': qa['question'],
                    'ç­”æ¡ˆ': qa['answer'],
                    'æª¢ç´¢æ™‚é–“ (ç§’)': qa['retrieval_time_sec'],
                    'ç”Ÿæˆæ™‚é–“ (ç§’)': qa['generation_time_sec'],
                    'ç”Ÿæˆé€Ÿåº¦ (tokens/ç§’)': qa['tokens_per_sec'],
                    'ç­”æ¡ˆé•·åº¦ (å­—å…ƒ)': qa['answer_length'],
                    'PDF æ–‡ä»¶': ', '.join(r.get('pdf_files', []))
                })
        
        if qa_data:
            df_qa = pd.DataFrame(qa_data)
            df_qa.to_excel(writer, sheet_name='å•é¡Œèˆ‡ç­”æ¡ˆ', index=False)
    
    print(f"âœ… Excel å ±å‘Šå·²ä¿å­˜: {output_file}")


def main():
    """ä¸»å‡½æ•¸"""
    print("\n" + "=" * 80)
    print("Qwen RAG å®Œæ•´æ•ˆèƒ½æ¸¬è©¦")
    print("=" * 80 + "\n")
    
    # é…ç½®
    PDF_FOLDER = "PDF"  # PDF è³‡æ–™å¤¾è·¯å¾‘

    QUESTIONS = None
    try:
        import json
        qpath = Path(os.getcwd()) / "test_questions.json"
        if qpath.exists():
            with open(qpath, 'r', encoding='utf-8') as f:
                items = json.load(f)
                # items é æœŸç‚º list of {"question": ..., ...}
                QUESTIONS = [it.get('question', '').strip() for it in items if it.get('question')]
    except Exception:
        QUESTIONS = None

    if not QUESTIONS:
        QUESTIONS = [
            "å ±å‘Šä¸­æåˆ°å…¨çƒæ­£å¾ã€Œå…¨çƒåŒ–ã€è½‰å‘ã€Œå†å…¨çƒåŒ–ã€ï¼ˆRe-globalizationï¼‰ï¼Œä¸”é¢è‡¨ã€Œå·æ™® 2.0ã€å¸¶ä¾†çš„é—œç¨…èˆ‡ä¾›æ‡‰éˆé‡çµ„å£“åŠ›ã€‚è«‹åˆ†æé€™ç¨®åœ‹éš›å±€å‹¢å¦‚ä½•å…·é«”å½±éŸ¿å°ç£åœ¨ã€ŒåŠå°é«”ã€èˆ‡ã€Œå·¥å…·æ©Ÿã€é€™å…©å€‹é—œéµç”¢æ¥­çš„æŠ€è¡“å¸ƒå±€ç­–ç•¥ï¼Ÿæ”¿åºœåˆæå‡ºäº†å“ªäº›å…·é«”çš„ã€Œä¾›æ‡‰éˆéŸŒæ€§ã€æˆ–ã€Œè‡ªä¸»åŒ–ã€æªæ–½ä¾†æ‡‰å°é€™äº›å¤–éƒ¨è¡æ“Šï¼Ÿ",
            "ç™½çš®æ›¸å¼·èª¿ã€Œæ•¸ä½è½‰å‹ã€èˆ‡ã€Œæ·¨é›¶è½‰å‹ã€æ˜¯å°ç£ç”¢æ¥­çš„é›™è»¸æ ¸å¿ƒã€‚è«‹è©³ç´°èªªæ˜åœ¨ã€ŒæåŒ–é ˜åŸŸã€æˆ–ã€Œæ™ºæ…§è£½é€ ã€ä¸­ï¼Œå¦‚ä½•å…·é«”åˆ©ç”¨ã€ŒAI æŠ€è¡“ã€ï¼ˆå¦‚ç”Ÿæˆå¼ AIã€æ©Ÿå™¨å­¸ç¿’ï¼‰ä¾†åŒæ™‚é”æˆã€Œè£½ç¨‹æ•ˆç‡æå‡ã€èˆ‡ã€Œç¯€èƒ½æ¸›ç¢³ã€é€™å…©å€‹çœ‹ä¼¼è¡çªçš„ç›®æ¨™ï¼Ÿè«‹èˆ‰å‡ºå ±å‘Šä¸­æåˆ°çš„å…·é«”æŠ€è¡“æ¡ˆä¾‹ï¼ˆä¾‹å¦‚åŒ–å·¥è£½ç¨‹æˆ–é‡‘å±¬åŠ å·¥ï¼‰ä½è­‰ã€‚",
            "é‡å°ã€Œäº”å¤§ä¿¡è³´ç”¢æ¥­ã€ä¸­çš„æ¬¡ä¸–ä»£é€šè¨Šï¼Œå ±å‘Šæå‡ºäº†ã€Œåœ°é¢ã€èˆ‡ã€Œéåœ°é¢ï¼ˆNTNï¼‰ã€ç¶²è·¯çš„æ•´åˆé¡˜æ™¯ã€‚è«‹æ·±å…¥è§£é‡‹å°ç£åœ¨ã€Œä½è»Œè¡›æ˜Ÿï¼ˆLEOï¼‰ã€åœ°é¢è¨­å‚™çš„é—œéµæŠ€è¡“ç¼ºå£ç‚ºä½•ï¼ˆå¦‚å°„é »æ™¶ç‰‡ã€ç›¸æ§é™£åˆ—å¤©ç·šï¼‰ï¼Ÿä»¥åŠã€Œè»Ÿé«”å®šç¾©ç„¡ç·šé›»ï¼ˆSDRï¼‰ã€æŠ€è¡“å¦‚ä½•åœ¨å»ºæ§‹é€™ç¨®ã€Œ3D ç«‹é«”é€šè¨Šç¶²è·¯ã€ä¸­æ‰®æ¼”æ ¸å¿ƒè§’è‰²ï¼Ÿ",
            "åœ¨æ‘©çˆ¾å®šå¾‹é€¼è¿‘æ¥µé™çš„èƒŒæ™¯ä¸‹ï¼Œç™½çš®æ›¸æŒ‡å‡ºã€Œç•°è³ªæ•´åˆå°è£ã€èˆ‡ã€ŒçŸ½å…‰å­ï¼ˆCPOï¼‰ã€æ˜¯æœªä¾†çš„é—œéµã€‚è«‹åˆ†æå°ç£åœ¨ç™¼å±•é€™äº›æŠ€è¡“æ™‚ï¼Œé¢è‡¨äº†å“ªäº›ã€Œè¨­å‚™ã€èˆ‡ã€Œææ–™ã€ä¸Šçš„è‡ªä¸»åŒ–æŒ‘æˆ°ï¼ˆä¾‹å¦‚æ•£ç†±åŸºæ¿ææ–™ã€æª¢æ¸¬è¨­å‚™ï¼‰ï¼Ÿæ”¿åºœçš„ã€Œæ™¶å‰µå°ç£æ–¹æ¡ˆã€èˆ‡ç›¸é—œç§‘å°ˆè¨ˆç•«åˆæ˜¯å¦‚ä½•å”åŠ©å» å•†çªç ´é€™äº›è¢«åœ‹å¤–å¤§å» å£Ÿæ–·çš„ç“¶é ¸ï¼Ÿ",
            "é‡å°ã€Œå¥åº·å°ç£ã€çš„é¡˜æ™¯ï¼Œç™½çš®æ›¸ä¸­æåˆ°çš„ã€Œæ–°è—¥é–‹ç™¼ã€èˆ‡ã€Œé†«ç™‚å™¨æã€å¦‚ä½•æ“ºè„«å‚³çµ±ç ”ç™¼æ¨¡å¼ï¼Ÿè«‹å…·é«”èªªæ˜ã€ŒAI é‹ç®—ã€èˆ‡ã€Œç”Ÿé†«æ™¶ç‰‡ã€æŠ€è¡“å¦‚ä½•è¢«æ‡‰ç”¨æ–¼ç¸®çŸ­æ–°è—¥é–‹ç™¼é€±æœŸï¼ˆå¦‚ mRNA è—¥ç‰©ï¼‰ï¼Œä»¥åŠå¯¦ç¾ã€Œéä¾µå…¥å¼ã€æˆ–ã€Œå±…å®¶åŒ–ã€çš„ç²¾æº–é†«ç™‚ï¼ˆå¦‚çœ¼ç§‘æ»´åŠ‘æˆ–é«˜é½¡ç…§è­·ï¼‰ï¼Ÿ"
        ]
    OUTPUT_JSON = "rag_performance_test_results.json"
    OUTPUT_EXCEL = "RAGæ•ˆèƒ½æ¸¬è©¦å ±å‘Š.xlsx"
    
    # æ¸¬è©¦æ¨¡å‹åˆ—è¡¨ï¼ˆå·²ä¸‹è¼‰çš„æ‰€æœ‰æ¨¡å‹ï¼‰
    MODELS = [
        "Qwen/Qwen2-0.5B-Instruct",
        "Qwen/Qwen2-1.5B-Instruct",
        "Qwen/Qwen2.5-1.5B-Instruct",
        "Qwen/Qwen2.5-3B-Instruct",
        "Qwen/Qwen3-4B-Instruct-2507",
        # 7B æ¨¡å‹éœ€è¦æ›´å¤š VRAMï¼Œå¦‚æœé¡¯å¡è¶³å¤ å¯å–æ¶ˆè¨»è§£ï¼š
         "Qwen/Qwen2-7B-Instruct",
         "Qwen/Qwen2.5-7B-Instruct",
    ]
    
    # æª¢æŸ¥ PDF è³‡æ–™å¤¾
    if not os.path.exists(PDF_FOLDER):
        print(f"âŒ éŒ¯èª¤: PDF è³‡æ–™å¤¾ä¸å­˜åœ¨: {PDF_FOLDER}")
        print(f"è«‹å‰µå»ºè³‡æ–™å¤¾ä¸¦æ”¾å…¥ PDF æ–‡ä»¶")
        return
    
    pdf_files = list(Path(PDF_FOLDER).glob("*.pdf"))
    if not pdf_files:
        print(f"âŒ éŒ¯èª¤: åœ¨ {PDF_FOLDER} ä¸­æ‰¾ä¸åˆ° PDF æ–‡ä»¶")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(pdf_files)} å€‹ PDF æ–‡ä»¶:")
    for pdf in pdf_files:
        print(f"   - {pdf.name}")
    print()
    
    # åŸ·è¡Œæ¸¬è©¦
    all_results = []
    
    for i, model_name in enumerate(MODELS, 1):
        print(f"\n{'='*80}")
        print(f"æ¸¬è©¦é€²åº¦: {i}/{len(MODELS)}")
        print(f"{'='*80}\n")
        
        try:
            result = test_rag_with_pdf(model_name, PDF_FOLDER, QUESTIONS)
            all_results.append(result)
            
            # ä¿å­˜ä¸­é–“çµæœ
            with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            print(f"\nâŒ æ¸¬è©¦å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
        
        # æ¸…ç†è¨˜æ†¶é«”
        if i < len(MODELS):
            print("\nğŸ§¹ æ¸…ç†è¨˜æ†¶é«”...")
            clear_memory()
            time.sleep(3)
    
    # ç”Ÿæˆå ±å‘Š
    print("\n" + "=" * 80)
    print("ç”Ÿæˆå ±å‘Š")
    print("=" * 80)
    
    # JSON å ±å‘Š
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    print(f"âœ… JSON å ±å‘Šå·²ä¿å­˜: {OUTPUT_JSON}")
    
    # Excel å ±å‘Š
    generate_excel_report(all_results, OUTPUT_EXCEL)
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 80)
    print("æ¸¬è©¦æ‘˜è¦")
    print("=" * 80 + "\n")
    
    for result in all_results:
        if result.get('qa_success'):
            print(f"âœ… {result['model_name']}")
            print(f"   è¨­å‚™: {result['device']}")
            print(f"   å•é¡Œæ•¸é‡: {result.get('total_questions', 0)}")
            print(f"   ç¸½è€—æ™‚: {result.get('qa_total_time_sec', 0):.2f} ç§’")
            print(f"   å¹³å‡ç”Ÿæˆé€Ÿåº¦: {result.get('avg_tokens_per_sec', 0):.2f} tokens/ç§’")
            print(f"   å³°å€¼ VRAM: {result['final_stats']['peak_vram_mb']} MB")
            print(f"   RAG é¡å¤–æˆæœ¬: {result.get('rag_overhead_mb', 0)} MB")
            print()
        else:
            print(f"âŒ {result['model_name']} - æ¸¬è©¦å¤±æ•—")
            print()
    
    print("=" * 80)
    print("æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")
    print(f"è©³ç´°å ±å‘Š: {OUTPUT_EXCEL}")
    print("=" * 80)


if __name__ == "__main__":
    main()
