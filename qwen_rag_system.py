"""
Qwen RAG System - A document Q&A system using Qwen models and RAG
æ”¯æ´è®€å–èªªæ˜æ›¸ã€ä½¿ç”¨æ‰‹å†Šå’Œé›»å­æ›¸ï¼Œä¸¦å›ç­”ç›¸é—œå•é¡Œ
"""

import os
from pathlib import Path
from typing import List
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from huggingface_hub import snapshot_download
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
import warnings
warnings.filterwarnings('ignore')

class QwenRAGSystem:
    """Qwenæ¨¡å‹çµåˆRAGçš„å•ç­”ç³»çµ±"""
    
    def __init__(self, model_name: str = "Qwen/Qwen2.5-3B-Instruct", device: str = "auto"):
        """
        åˆå§‹åŒ–Qwen RAGç³»çµ±
        
        Args:
            model_name: Hugging Faceä¸Šçš„Qwenæ¨¡å‹åç¨±
            device: é‹è¡Œè¨­å‚™ ('cuda', 'cpu', æˆ– 'auto')
        """
        print(f"ğŸš€ æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_name}")
        
        # è¨­å®šè¨­å‚™
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        print(f"ğŸ“± ä½¿ç”¨è¨­å‚™: {self.device}")
        
        # æº–å‚™æœ¬åœ°æ¨¡å‹å„²å­˜è·¯å¾‘ (./model/<repo_id æ›¿æ›ç‚º __>)
        root_dir = Path(os.getcwd()) / "model"
        root_dir.mkdir(exist_ok=True)
        safe_name = model_name.replace('/', '__')
        local_model_dir = root_dir / safe_name
        
        # è‹¥æœ¬åœ°å°šæœªå­˜åœ¨å‰‡ä¸‹è¼‰
        if not local_model_dir.exists() or not any(local_model_dir.glob('*.bin')) and not any(local_model_dir.glob('*.safetensors')):
            print(f"ğŸš€ ä¸‹è¼‰æ¨¡å‹åˆ° {local_model_dir} ...")
            os.environ.setdefault("HF_HUB_DISABLE_SYMLINKS", "1")  # Windows é¿å… symlink å•é¡Œ
            snapshot_download(
                repo_id=model_name,
                local_dir=str(local_model_dir),
                local_dir_use_symlinks=False,
                resume_download=True
            )
        else:
            print(f"ğŸ“¦ æœ¬åœ°æ¨¡å‹å·²å­˜åœ¨: {local_model_dir}")
        
        # è¼‰å…¥tokenizerå’Œæ¨¡å‹ï¼ˆå¾æœ¬åœ°è³‡æ–™å¤¾ï¼‰
        self.tokenizer = AutoTokenizer.from_pretrained(
            str(local_model_dir),
            trust_remote_code=True
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            str(local_model_dir),
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None,
            trust_remote_code=True
        )
        
        if self.device == "cpu":
            self.model = self.model.to(self.device)
        
        print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ")
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        print("ğŸ”§ æ­£åœ¨è¼‰å…¥Embeddingæ¨¡å‹...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        
        # åˆå§‹åŒ–å‘é‡å­˜å„²
        self.vectorstore = None
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        
        print("âœ… ç³»çµ±åˆå§‹åŒ–å®Œæˆ\n")
    
    def load_documents(self, file_paths: List[str]):
        """
        è¼‰å…¥æ–‡ä»¶ä¸¦å»ºç«‹å‘é‡æ•¸æ“šåº«
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾‘åˆ—è¡¨ï¼ˆæ”¯æ´PDFå’ŒTXTï¼‰
        """
        print("ğŸ“š é–‹å§‹è¼‰å…¥æ–‡ä»¶...")
        documents = []
        
        for file_path in file_paths:
            print(f"  ğŸ“„ è®€å–: {os.path.basename(file_path)}")
            
            if file_path.endswith('.pdf'):
                loader = PyPDFLoader(file_path)
            elif file_path.endswith('.txt'):
                loader = TextLoader(file_path, encoding='utf-8')
            else:
                print(f"  âš ï¸  ä¸æ”¯æ´çš„æ–‡ä»¶æ ¼å¼: {file_path}")
                continue
            
            docs = loader.load()
            documents.extend(docs)
        
        print(f"âœ… å…±è¼‰å…¥ {len(documents)} å€‹æ–‡ä»¶æ®µè½")
        
        # åˆ†å‰²æ–‡æœ¬
        print("âœ‚ï¸  æ­£åœ¨åˆ†å‰²æ–‡æœ¬...")
        splits = self.text_splitter.split_documents(documents)
        print(f"âœ… åˆ†å‰²æˆ {len(splits)} å€‹æ–‡æœ¬å¡Š")
        
        # å»ºç«‹å‘é‡æ•¸æ“šåº«
        print("ğŸ—„ï¸  æ­£åœ¨å»ºç«‹å‘é‡æ•¸æ“šåº«...")
        self.vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=self.embeddings,
            persist_directory="./chroma_db"
        )
        print("âœ… å‘é‡æ•¸æ“šåº«å»ºç«‹å®Œæˆ\n")
    
    def retrieve_context(self, query: str, k: int = 3) -> str:
        """
        æ ¹æ“šæŸ¥è©¢æª¢ç´¢ç›¸é—œä¸Šä¸‹æ–‡
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            k: è¿”å›æœ€ç›¸é—œçš„kå€‹æ–‡æœ¬å¡Š
            
        Returns:
            æ‹¼æ¥çš„ä¸Šä¸‹æ–‡æ–‡æœ¬
        """
        if self.vectorstore is None:
            return ""
        
        docs = self.vectorstore.similarity_search(query, k=k)
        context = "\n\n".join([doc.page_content for doc in docs])
        return context
    
    def generate_answer(self, query: str, context: str = None) -> str:
        """
        ä½¿ç”¨Qwenæ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ¶å•é¡Œ
            context: æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆå¯é¸ï¼‰
            
        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        if context:
            prompt = f"""æ ¹æ“šä»¥ä¸‹æ–‡ä»¶å…§å®¹å›ç­”å•é¡Œã€‚å¦‚æœæ–‡ä»¶ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹èª å¯¦åœ°èªªä¸çŸ¥é“ã€‚

æ–‡ä»¶å…§å®¹:
{context}

å•é¡Œ: {query}

ç­”æ¡ˆ:"""
        else:
            prompt = query
        
        # ä½¿ç”¨Qwençš„chatæ¨¡æ¿
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„åŠ©æ‰‹ï¼Œèƒ½å¤ æ ¹æ“šæä¾›çš„æ–‡ä»¶å…§å®¹æº–ç¢ºå›ç­”å•é¡Œã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
        
        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9,
                do_sample=True
            )
        
        response = self.tokenizer.decode(
            outputs[0][len(inputs.input_ids[0]):],
            skip_special_tokens=True
        )
        
        return response.strip()
    
    def ask(self, question: str, use_rag: bool = True) -> str:
        """
        å•ç­”æ¥å£
        
        Args:
            question: ç”¨æˆ¶å•é¡Œ
            use_rag: æ˜¯å¦ä½¿ç”¨RAGï¼ˆæª¢ç´¢å¢å¼·ï¼‰
            
        Returns:
            ç­”æ¡ˆ
        """
        if use_rag and self.vectorstore is not None:
            print(f"\nâ“ å•é¡Œ: {question}")
            print("ğŸ” æ­£åœ¨æª¢ç´¢ç›¸é—œå…§å®¹...")
            context = self.retrieve_context(question)
            print("ğŸ¤– æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
            answer = self.generate_answer(question, context)
        else:
            print(f"\nâ“ å•é¡Œ: {question}")
            print("ğŸ¤– æ­£åœ¨ç”Ÿæˆç­”æ¡ˆï¼ˆä¸ä½¿ç”¨RAGï¼‰...")
            answer = self.generate_answer(question)
        
        print(f"ğŸ’¡ ç­”æ¡ˆ: {answer}\n")
        return answer


def main():
    """ä¸»å‡½æ•¸ - ç¤ºä¾‹ç”¨æ³•"""
    
    print("=" * 60)
    print("Qwen RAG æ–‡ä»¶å•ç­”ç³»çµ±")
    print("=" * 60 + "\n")
    
    # åˆå§‹åŒ–ç³»çµ±ï¼ˆä½¿ç”¨è¼ƒå°çš„Qwenæ¨¡å‹ï¼‰
    # å¯é¸çš„7Bä»¥ä¸‹æ¨¡å‹:
    # - Qwen/Qwen2.5-1.5B-Instruct
    # - Qwen/Qwen2.5-3B-Instruct
    # - Qwen/Qwen2.5-7B-Instruct
    # - Qwen/Qwen2-1.5B-Instruct
    
    rag_system = QwenRAGSystem(model_name="Qwen/Qwen2.5-3B-Instruct")
    
    print("\nğŸ“– ä½¿ç”¨èªªæ˜:")
    print("1. å°‡æ‚¨çš„PDFæˆ–TXTæ–‡ä»¶æ”¾åœ¨ç•¶å‰ç›®éŒ„")
    print("2. ç³»çµ±æœƒè‡ªå‹•è¼‰å…¥ä¸¦å»ºç«‹çŸ¥è­˜åº«")
    print("3. æ‚¨å¯ä»¥é–‹å§‹æå•ç›¸é—œå•é¡Œ")
    print("\n" + "=" * 60 + "\n")
    
    # ç¤ºä¾‹: è¼‰å…¥æ–‡ä»¶
    # è«‹å°‡ä»¥ä¸‹è·¯å¾‘æ›¿æ›ç‚ºæ‚¨å¯¦éš›çš„æ–‡ä»¶è·¯å¾‘
    # file_paths = [
    #     "manual.pdf",      # ä½¿ç”¨æ‰‹å†Š
    #     "guide.pdf",       # èªªæ˜æ›¸
    #     "ebook.txt"        # é›»å­æ›¸
    # ]
    # rag_system.load_documents(file_paths)
    
    # äº’å‹•å¼å•ç­”
    print("ğŸ’¬ é–‹å§‹äº’å‹•å¼å•ç­”ï¼ˆè¼¸å…¥ 'quit' æˆ– 'exit' çµæŸï¼‰\n")
    
    while True:
        question = input("æ‚¨çš„å•é¡Œ: ").strip()
        
        if question.lower() in ['quit', 'exit', 'é€€å‡º', 'çµæŸ']:
            print("\nğŸ‘‹ å†è¦‹ï¼")
            break
        
        if not question:
            continue
        
        try:
            answer = rag_system.ask(question)
        except Exception as e:
            print(f"âŒ éŒ¯èª¤: {e}\n")


if __name__ == "__main__":
    main()
